{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48539a86",
   "metadata": {},
   "source": [
    "match_details\n",
    "a row for every players performance in a match\n",
    "matches\n",
    "a row for every match\n",
    "medals_matches_players\n",
    "a row for every medal type a player gets in a match\n",
    "medals\n",
    "a row for every medal type\n",
    "Your goal is to make the following things happen:\n",
    "\n",
    "Build a Spark job that\n",
    "Disabled automatic broadcast join with spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "Explicitly broadcast JOINs medals and maps\n",
    "Bucket join match_details, matches, and medal_matches_players on match_id with 16 buckets\n",
    "Aggregate the joined data frame to figure out questions like:\n",
    "Which player averages the most kills per game?\n",
    "Which playlist gets played the most?\n",
    "Which map gets played the most?\n",
    "Which map do players get the most Killing Spree medals on?\n",
    "With the aggregated data set\n",
    "Try different .sortWithinPartitions to see which has the smallest data size (hint: playlists and maps are both very low cardinality)\n",
    "Save these as .py files and submit them this way!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session with optimizations\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark SQL NBA game analytics\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.sources.bucketing.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.sources.bucketing.autoBucketedScan.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Disable automatic broadcast join threshold as required\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "df_game_details = spark.read.jdbc(url=url, table=\"game_details\", properties=properties)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c494a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maps = spark.read.csv(\n",
    "    \"Echooed/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/maps.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_maps.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a22b468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(spark, url, properties):\n",
    "    \"\"\"Load all required datasets\"\"\"\n",
    "    \n",
    "    print(\"Loading all datasets...\")\n",
    "    \n",
    "    # Load NBA data from JDBC\n",
    "    print(\"Loading NBA game details from database...\")\n",
    "    df_game_details = spark.read.jdbc(url=url, table=\"game_details\", properties=properties)\n",
    "    \n",
    "    # Load gaming datasets from CSV files\n",
    "    print(\"Loading gaming datasets from CSV files...\")\n",
    "    \n",
    "    # Load maps data\n",
    "    df_maps = spark.read.csv(\n",
    "        \"/home/dataspiro/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/maps.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    # Load other gaming datasets (update paths as needed)\n",
    "    try:\n",
    "        df_match_details = spark.read.csv(\n",
    "            \"/home/dataspiro/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/match_details.csv\",\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        \n",
    "        df_matches = spark.read.csv(\n",
    "            \"/home/dataspiro/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/matches.csv\",\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        \n",
    "        df_medals_matches_players = spark.read.csv(\n",
    "            \"/home/dataspiro/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/medals_matches_players.csv\",\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        \n",
    "        df_medals = spark.read.csv(\n",
    "            \"/home/dataspiro/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/medals.csv\",\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        \n",
    "        print(\"All gaming datasets loaded successfully!\")\n",
    "        gaming_data_available = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Gaming datasets not available: {e}\")\n",
    "        print(\"Will proceed with NBA analysis only\")\n",
    "        gaming_data_available = False\n",
    "        df_match_details = None\n",
    "        df_matches = None\n",
    "        df_medals_matches_players = None\n",
    "        df_medals = None\n",
    "    \n",
    "    return {\n",
    "        'game_details': df_game_details,\n",
    "        'maps': df_maps,\n",
    "        'match_details': df_match_details,\n",
    "        'matches': df_matches,\n",
    "        'medals_matches_players': df_medals_matches_players,\n",
    "        'medals': df_medals,\n",
    "        'gaming_data_available': gaming_data_available\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fa24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = spark.read.csv(\n",
    "    \"/home/dataspiro/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/match_details.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df_test.show(5)\n",
    "\n",
    "\n",
    "# df_match_details = spark.read.csv(\n",
    "#             \"Echooed/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/match_details.csv\",\n",
    "#             header=True,\n",
    "#             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81216ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/22 10:48:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Create and configure Spark session with optimizations\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark SQL NBA game analytics\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.sources.bucketing.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.sources.bucketing.autoBucketedScan.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Disable automatic broadcast join threshold as required\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL NBA game analytics\") \\\n",
    "    .config(\"spark.jars\", \"/home/your-user/jars/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see what columns you have\n",
    "print(\"Your NBA data columns:\")\n",
    "print(df_game_details.columns)\n",
    "df_game_details.printSchema()\n",
    "df_game_details.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3dc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tables_and_optimizations(spark):\n",
    "    \"\"\"Set up NBA tables with proper optimizations using Spark SQL\"\"\"\n",
    "    \n",
    "    print(\"Setting up NBA database and tables...\")\n",
    "    \n",
    "    # Create database\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS nba_analysis\")\n",
    "    spark.sql(\"USE nba_analysis\")\n",
    "    \n",
    "    # Create game_details table with bucketing\n",
    "    df_game_details.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .bucketBy(16, \"game_id\") \\\n",
    "        .sortBy(\"game_id\", \"player_id\") \\\n",
    "        .saveAsTable(\"bucketed_game_details\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aefc4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Spark\n",
    "spark = create_spark_session()\n",
    "\n",
    "# 2. Define Postgres connection info\n",
    "url = \"jdbc:postgresql://localhost:5434/postgres\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the jar is loaded\n",
    "\n",
    "jars = spark.sparkContext._jsc.sc().listJars()\n",
    "print(jars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b995e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with PostgreSQL JDBC driver\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NBA Database Connection\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"NBA tables created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data validation\n",
    "total_records = spark.sql(\"SELECT COUNT(*) as total FROM bucketed_game_details\").collect()[0]['total']\n",
    "total_games = spark.sql(\"SELECT COUNT(DISTINCT game_id) as games FROM bucketed_game_details\").collect()[0]['games']\n",
    "total_players = spark.sql(\"SELECT COUNT(DISTINCT player_id) as players FROM bucketed_game_details\").collect()[0]['players']\n",
    "    \n",
    "print(f\"Data loaded: {total_records:,} records, {total_games:,} games, {total_players:,} unique players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c4630d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_match_details' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load table\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# df = spark.read.jdbc(url=url, table=\"game_details\", properties=properties)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdf_match_details\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_match_details' is not defined"
     ]
    }
   ],
   "source": [
    "# Load table\n",
    "# df = spark.read.jdbc(url=url, table=\"game_details\", properties=properties)\n",
    "\n",
    "df_match_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77e0a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/home/dataspiro/data-engineer-handbook/bootcamp/materials/3-spark-fundamentals/data/match_details.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482d683e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: write Spark DataFrame to PostgreSQL\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_matches\u001b[49m\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://localhost:5434/postgres\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_matches' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: write Spark DataFrame to PostgreSQL\n",
    "df_matches.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5434/postgres\") \\\n",
    "    .option(\"dbtable\", \"matches\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\")  \\\n",
    "    .save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
